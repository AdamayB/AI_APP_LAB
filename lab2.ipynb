{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c9541f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\student\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\student\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5c05a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Next Generation Of Large Language ModelsSubscribe to newslettersSubscribeSign InBETAThis is a BETA experience. You may opt-out by clicking hereMore From ForbesJan 29, 2024,05:33pm ESTCEO Replika A Leader In Virtual Companions Shares Lessons LearnedJan 29, 2024,08:30am ESTIs 2024 The Year Of Investor Restraint And Startup Resilience? 8 Experts Weigh InJan 29, 2024,07:53am ESTMitigating Legal Risks When Using Generative AIJan 29, 2024,06:25am ESTThe AI Caddie: Revolutionizing Golf With AI TechnologyJan 28, 2024,02:54pm ESTFrom Davos To Dominance: How AI Is Rewriting Our Planet and BusinessJan 28, 2024,01:38pm ESTRevolutionizing Marketing: The Convergence Of Data Science And AIJan 28, 2024,06:15am ESTCan Generative AI Convince Medical Doctors They Are Wrong When They Are Right And Right When They Are WrongJan 27, 2024,05:47pm ESTFacing The AI Generative Reality HeadOn: Are You Really Ready?Edit StoryForbesInnovationAIEditors' PickThe Next Generation Of Large Language ModelsRob ToewsContributorOpinions expressed by Forbes Contributors are their own.I write about the big picture of artificial intelligence.FollowingClick to save this article.You'll be asked to sign into your Forbes account.Got itFeb 7, 2023,11:00am ESTShare to FacebookShare to TwitterShare to LinkedinOpenAI CEO Sam Altman (left) and Meta AI chief Yann LeCun (right) have differing views on the future ... [+] of large language models.Photo credit: Getty Images\n",
      "In case you haven’t heard, artificial intelligence is the hot new thing.\n",
      "\n",
      "Generative AI seems to be on the lips of every venture capitalist, entrepreneur, Fortune 500 CEO and journalist these days, from Silicon Valley to Davos.\n",
      "\n",
      "\n",
      "To those who started paying real attention to AI in 2022, it may seem that technologies like ChatGPT and Stable Diffusion came out of nowhere to take the world by storm. They didn’t.\n",
      "\n",
      "Back in 2020, we wrote an article in this column predicting that generative AI would be one of the pillars of the next generation of artificial intelligence.\n",
      "\n",
      "Since at least the release of GPT-2 in 2019, it has been clear to those working in the field that generative language models were poised to unleash vast economic and societal transformation. Similarly, while text-to-image models only captured the public’s attention last summer, the technology’s ascendance has appeared inevitable since OpenAI released the original DALL-E in January 2021. (We wrote an article making this argument days after the release of the original DALL-E.)\n",
      "\n",
      "By this same token, it is important to remember that the current state of the art in AI is far from an end state for AI’s capabilities. On the contrary, the frontiers of artificial intelligence have never advanced more rapidly than they are right now. As amazing as ChatGPT seems to us at the moment, it is a mere stepping stone to what comes next.\n",
      "What will the next generation of large language models (LLMs) look like? The answer to this question is already out there, under development at AI startups and research groups at this very moment.\n",
      "MORE FROMFORBES ADVISORBest High-Yield Savings Accounts Of September 2023ByKevin PayneContributorBest 5% Interest Savings Accounts of September 2023ByCassidy HortonContributor\n",
      "This article highlights three emerging areas that will help define the next wave of innovation in generative AI and LLMs. For those looking to remain ahead of the curve in this fast-changing world—read on.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1) Models that can generate their own training data to improve themselves.\n",
      "Consider how humans think and learn. We collect knowledge and perspective from external sources of information—say, by reading a book. But we also generate novel ideas and insights on our own, by reflecting on a topic or thinking through a problem in our minds. We are able to deepen our understanding of the world via internal reflection and analysis not directly tied to any new external input.\n",
      "A new avenue of AI research seeks to enable large language models to do something analogous, effectively bootstrapping their own intelligence.\n",
      "As part of their training, today’s LLMs ingest much of the world’s accumulated written information (e.g., Wikipedia, books, news articles). What if these models, once trained, could use all the knowledge that they have absorbed from these sources to produce new written content—and then use that content as additional training data in order to improve themselves? Initial work suggests that this approach may be possible—and powerful.\n",
      "In one recent research effort, aptly titled “Large Language Models Can Self-Improve,” a group of Google researchers built an LLM that can come up with a set of questions, generate detailed answers to those questions, filter its own answers for the most high-quality output, and then fine-tune itself on the curated answers. Remarkably, this leads to new state-of-the-art performance on various language tasks. For instance, the model’s performance increased from 74.2% to 82.1% on GSM8K and from 78.2% to 83.0% on DROP, two popular benchmarks used to evaluate LLM performance.\n",
      "Another recent work builds on an important LLM method called “instruction fine-tuning,” which lies at the core of products like ChatGPT. Whereas ChatGPT and other instruction fine-tuned models rely on human-written instructions, this research group built a model that can generate its own natural language instructions and then fine-tune itself on those instructions. The performance gains are dramatic: this method improves the performance of the base GPT-3 model by 33%, nearly matching the performance of OpenAI’s own instruction-tuned model.\n",
      "In a thematically related work, researchers from Google and Carnegie Mellon show that if a large language model, when presented with a question, first recites to itself what it knows about the topic before responding, it provides more accurate and sophisticated responses. This can be loosely analogized to a human in conversation who, rather than blurting out the first thing that comes to mind on a topic, searches her memory and reflects on her beliefs before sharing a perspective.\n",
      "When people first hear about this line of research, a conceptual objection often arises—isn’t this all circular? How can a model produce data that the model can then consume to improve itself? If the new data came from the model in the first place, shouldn’t the “knowledge” or “signal” that it contains already be incorporated in the model?\n",
      "This objection makes sense if we conceive of large language models as databases, storing information from their training data and reproducing it in different combinations when prompted. But—uncomfortable or even eerie as it may sound—we are better off instead conceiving of large language models along the lines of the human brain (no, the analogy is of course not perfect!).\n",
      "We humans ingest a tremendous amount of data from the world that alters the neural connections in our brains in imponderable, innumerable ways. Through introspection, writing, conversation—sometimes just a good night’s sleep—our brains can then produce new insights that had not previously been in our minds nor in any information source out in the world. If we internalize these new insights, they can make us wiser.\n",
      "The idea that LLMs can generate their own training data is particularly important in light of the fact that the world may soon run out of text training data. This is not yet a widely appreciated problem, but it is one that many AI researchers are worried about.\n",
      "By one estimate, the world’s total stock of usable text data is between 4.6 trillion and 17.2 trillion tokens. This includes all the world’s books, all scientific papers, all news articles, all of Wikipedia, all publicly available code, and much of the rest of the internet, filtered for quality (e.g., webpages, blogs, social media). Another recent estimate puts the total figure at 3.2 trillion tokens.\n",
      "DeepMind’s Chinchilla, one of today’s leading LLMs, was trained on 1.4 trillion tokens.\n",
      "In other words, we may be well within one order of magnitude of exhausting the world’s entire supply of useful language training data.\n",
      "If large language models are able to generate their own training data and use it to continue self-improving, this could render irrelevant the looming data shortage. It would represent a mind-bending leap forward for LLMs.\n",
      "\n",
      "2) Models that can fact-check themselves.\n",
      "A popular narrative these days is that ChatGPT and conversational LLMs like it are on the verge of replacing Google Search as the world’s go-to source for information, disrupting the once-mighty tech giant like Blockbuster or Kodak were disrupted before it.\n",
      "This narrative badly oversimplifies things. LLMs as they exist today will never replace Google Search. Why not? In short, because today’s LLMs make stuff up.\n",
      "As powerful as they are, large language models regularly produce inaccurate, misleading or false information (and present it confidently and convincingly).\n",
      "Examples abound of ChatGPT’s “hallucinations” (as these misstatements are referred to). This is not to single out ChatGPT; every generative language model in existence today hallucinates in similar ways.\n",
      "To give a few examples: it recommends books that don’t exist; it insists that the number 220 is less than 200; it is unsure whether Abraham Lincoln’s assassin was on the same continent as Lincoln at the time of the assassination; it provides plausible-sounding but incorrect explanations of concepts like Bayes’ Theorem.\n",
      "Most users will not accept a search engine that gets basic facts like these wrong some of the time; even 99% accuracy will not be good enough for broad market adoption. OpenAI CEO Sam Altman himself acknowledges this, recently cautioning: “ChatGPT is incredibly limited, but good enough at some things to create a misleading impression of greatness. It's a mistake to be relying on it for anything important right now.”\n",
      "It is an open question whether LLMs’ hallucination problem can be solved via incremental improvements to existing architectures, or whether a more fundamental paradigm shift in AI methodologies will be necessary to give AI common sense and real understanding. Deep learning pioneer Yann LeCun, for one, believes the latter. LeCun’s contrarian perspective may prove correct; time will tell.\n",
      "In the nearer term, though, a set of promising innovations offers to at least mitigate LLMs’ factual unreliability. These new methods will play an essential role in preparing LLMs for widespread real-world deployment.\n",
      "Two related capabilities lie at the heart of current efforts to make language models more accurate: (1) the ability for LLMs to retrieve information from external sources, and (2) the ability for LLMs to provide references and citations for the information they provide.\n",
      "ChatGPT is limited to the information that is already stored inside of it, captured in its static weights. (This is why it is not able to discuss events that occurred after 2021, when the model was trained.) Being able to pull in information from external sources will empower LLMs to access the most accurate and up-to-date information available, even when that information changes frequently (say, companies’ stock prices).\n",
      "Of course, having access to an external information source does not by itself guarantee that LLMs will retrieve the most accurate and relevant information. One important way for LLMs to increase transparency and trust with human users is to include references to the source(s) from which they retrieved the information. Such citations allow human users to audit the information source as needed in order to decide for themselves on its reliability.\n",
      "Important early work in this field includes models like REALM (from Google) and RAG (from Facebook), both published in 2020. With the rise of conversational LLMs in recent months, research in this area is now rapidly accelerating.\n",
      "Last year, OpenAI published a fine-tuned version of its GPT model named WebGPT that can browse the internet using Microsoft Bing in order to provide more accurate and in-depth responses to prompts. WebGPT navigates the internet much like a human does: it can submit search queries to Bing, follow links, scroll up and down on webpages, and use functions like Ctrl+F to find terms. When the model finds relevant information on the internet that it incorporates into its output, it provides citations so that the human user can see where the information came from.\n",
      "The results are encouraging: for the same query, WebGPT’s responses are preferred to responses written by human subjects 56% of the time and are preferred to the highest-rated responses on Reddit 69% of the time.\n",
      "DeepMind is also pursuing research along these lines. A few months ago, DeepMind published a new model named Sparrow. Like ChatGPT, Sparrow is dialogue-based; like WebGPT, it can search the internet for information and provide citations for its assertions. Sparrow builds on important earlier work out of DeepMind including SpaLM, RETRO and GopherCite.DeepMind's Sparrow model in action. As shown here, Sparrow provides quotations and links to support ... [+] its statements, increasing their accuracy and trustworthiness.Source: DeepMind\n",
      "The DeepMind researchers find that Sparrow’s citations are helpful and accurate 78% of the time—suggesting both that this research approach is promising and that the problem of LLM inaccuracy is far from solved.\n",
      "Younger startups including You.com and Perplexity have also recently launched LLM-powered conversational search interfaces with the ability to retrieve information from external sources and cite references. These products are available for public use today.\n",
      "LLMs’ greatest shortcoming is their unreliability, their stubborn tendency to confidently provide inaccurate information. Language models promise to reshape every sector of our economy, but they will never reach their full potential until this problem is addressed. Expect to see plenty of activity and innovation in this area in the months ahead.\n",
      "\n",
      "3) Massive sparse expert models.\n",
      "Today’s most prominent large language models all have effectively the same architecture.\n",
      "Meta AI chief Yann LeCun said recently: “In terms of underlying techniques, ChatGPT is not particularly innovative. It’s nothing revolutionary, although that’s the way it’s perceived in the public. It’s just that, you know, it’s well put together, it’s nicely done.”\n",
      "LeCun’s statement stirred up plenty of controversy and Twitter debate. But the simple fact is that he is correct, as no serious AI researcher would dispute.\n",
      "All of today’s well-known language models—e.g., GPT-3 from OpenAI, PaLM or LaMDA from Google, Galactica or OPT from Meta, Megatron-Turing from Nvidia/Microsoft, Jurassic-1 from AI21 Labs—are built in the same basic way. They are autoregressive, self-supervised, pre-trained, densely activated transformer-based models.\n",
      "To be sure, variations among these models exist: their size (parameter count), the data they are trained on, the optimization algorithm used, the batch size, the number of hidden layers, whether they are instruction fine-tuned, and so on. These variations can translate to meaningful performance differences. The core architectures, though, vary little.\n",
      "Yet momentum is building behind an intriguingly different architectural approach to language models known as sparse expert models. While the idea has been around for decades, it has only recently reemerged and begun to gain in popularity.\n",
      "All of the models mentioned above are dense. This means that every time the model runs, every single one of its parameters is used. Every time you submit a prompt to GPT-3, for instance, all 175 billion of the model’s parameters are activated in order to produce its response.\n",
      "But what if a model were able to call upon only the most relevant subset of its parameters in order to respond to a given query? This is the basic concept behind sparse expert models.\n",
      "The defining characteristic of sparse models is that they don’t activate all of their parameters for a given input, but rather only those parameters that are helpful in order to handle the input. Model sparsity thus decouples a model’s total parameter count from its compute requirements. This leads to sparse expert models’ key advantage: they can be both larger and less computationally demanding than dense models.\n",
      "Why are they called sparse expert models? Because sparse models can be thought of as consisting of a collection of “sub-models” that serve as experts on different topics. Depending on the prompt presented to the model, the most relevant experts within the model are activated while the other experts remain inactive. A prompt posed in Russian, for instance, would only activate the “experts” within a model that can understand and respond in Russian, efficiently bypassing the rest of the model.\n",
      "All of today’s largest LLMs are sparse. If you come across an LLM with more than 1 trillion parameters, you can safely assume that it is sparse. This includes Google’s Switch Transformer (1.6 trillion parameters), Google’s GLaM (1.2 trillion parameters) and Meta’s Mixture of Experts model (1.1 trillion parameters).\n",
      "“Much of the recent progress in AI has come from training larger and larger models,” said Mikel Artetxe, who led Meta’s research on sparse models before resigning to cofound a stealth LLM startup. “GPT-3, for instance, is more than 100 times larger than GPT-2. But when we double the size of a dense model, we also make it twice as slow. Sparse models allow us to train larger models without the increase in runtime.”\n",
      "Recent research on sparse expert models suggests that this architecture holds massive potential.\n",
      "GLaM, a sparse expert model developed last year by Google, is 7 times larger than GPT-3, requires two-thirds less energy to train, requires half as much compute for inference, and outperforms GPT-3 on a wide range of natural language tasks. Similar work on sparse models out of Meta has yielded similarly promising results.\n",
      "As the Meta researchers summarize: “We find that sparse models can achieve similar downstream task performance as dense models at a fraction of the compute. For models with relatively modest compute budgets, a sparse model can perform on par with a dense model that requires almost four times as much compute.”\n",
      "There is another benefit of sparse expert models that is worth mentioning: they are more interpretable than dense models.\n",
      "Interpretability—the ability for a human to understand why a model took the action that it did—is one of AI’s greatest weaknesses today. In general, today’s neural networks are uninterpretable “black boxes.” This can limit their usefulness in the real world, particularly in high-stakes settings like healthcare where human review is important.\n",
      "Sparse expert models lend themselves more naturally to interpretability than conventional models because a sparse model’s output is the result of an identifiable, discrete subset of parameters within the model—namely, the “experts” that were activated. The fact that humans can better extract understandable explanations from sparse models about their behavior may prove to be a decisive advantage for these models in real-world applications.\n",
      "Sparse expert models are not in widespread use today. They are less well understood and more technically complex to build than dense models. Yet considering their potential advantages, most of all their computational efficiency, don’t be surprised to see the sparse expert architecture become more prevalent in the world of LLMs going forward.\n",
      "In the words of Graphcore CTO Simon Knowles: “If an AI can do many things, it doesn’t need to access all of its knowledge to do one thing. It’s completely obvious. This is how your brain works, and it’s also how an AI ought to work. I’d be surprised if, by next year, anyone is building dense language models.”\n",
      "Note: The author is a Partner at Radical Ventures, which is an investor in You.com.\n",
      "Follow me on Twitter. Rob ToewsEditorial StandardsPrintReprints & Permissions\n"
     ]
    }
   ],
   "source": [
    "import requests, webbrowser\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import sys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?fbclid=IwAR3HM165sf71CJS_RSztDi2D4hQSHvUi93zoGsEW87PqOwhcHTGw3FwsciQ&sh=24132c8d18db\"\n",
    "\n",
    "html_content = requests.get(url).text\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "content=soup.text\n",
    "print(content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f57a0e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\student\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f97927ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a9e1000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\STUDENT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be6491bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\STUDENT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d761564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b86d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bfdbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88fb2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessor(content):\n",
    "    \n",
    "    #lower case\n",
    "    content=content.lower()\n",
    "    \n",
    "    #tokenize \n",
    "    content=nltk.word_tokenize(content)\n",
    "    \n",
    "    \n",
    "    #Remove spl chars\n",
    "    y=[]\n",
    "    for i in content:\n",
    "        if i.isalpha():\n",
    "            y.append(i)\n",
    "    \n",
    "    \n",
    "    content=y\n",
    "    y=[]\n",
    "    # remove stopwords : https://www.tutorialspoint.com/Removing-stop-words-with-NLTK-in-Python\n",
    "    for i in content:\n",
    "        if i not in stopwords.words('english') or i not in string.punctuation:\n",
    "            y.append(i)\n",
    "    \n",
    "    \n",
    "    #stemming reference:https://www.techtarget.com/searchenterpriseai/definition/stemming#:~:text=Stemming%20is%20the%20process%20of,natural%20language%20processing%20(NLP).\n",
    "    content=y\n",
    "    y=[]\n",
    "    stemmer=PorterStemmer()\n",
    "    for i in content:\n",
    "        y.append(stemmer.stem(i))\n",
    "    return \" \".join(y)\n",
    "    \n",
    "    #lemmatizer\n",
    "    ylem=[]\n",
    "    for i in y:\n",
    "        ylem.append(lemmatizer.lemmatize(i))\n",
    "    return ' '.join(ylem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "372a8fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the next gener of larg languag modelssubscrib to newsletterssubscribesign inbetathi is a beta experi you may by click heremor from forbesjan estceo replika a leader in virtual companion share lesson learnedjan esti the year of investor restraint and startup resili expert weigh injan estmitig legal risk when use gener aijan estth ai caddi revolution golf with ai technologyjan estfrom davo to domin how ai is rewrit our planet and businessjan estrevolution market the converg of data scienc and aijan estcan gener ai convinc medic doctor they are wrong when they are right and right when they are wrongjan estfac the ai gener realiti headon are you realli readi edit storyforbesinnovationaieditor pickth next gener of larg languag modelsrob toewscontributoropinion express by forb contributor are their write about the big pictur of artifici to save thi be ask to sign into your forb itfeb estshar to facebookshar to twittershar to linkedinopenai ceo sam altman left and meta ai chief yann lecun right have differ view on the futur of larg languag credit getti imag in case you haven t heard artifici intellig is the hot new thing gener ai seem to be on the lip of everi ventur capitalist entrepreneur fortun ceo and journalist these day from silicon valley to davo to those who start pay real attent to ai in it may seem that technolog like chatgpt and stabl diffus came out of nowher to take the world by storm they didn t back in we wrote an articl in thi column predict that gener ai would be one of the pillar of the next gener of artifici intellig sinc at least the releas of in it ha been clear to those work in the field that gener languag model were pois to unleash vast econom and societ transform similarli while model onli captur the public s attent last summer the technolog s ascend ha appear inevit sinc openai releas the origin in januari we wrote an articl make thi argument day after the releas of the origin by thi same token it is import to rememb that the current state of the art in ai is far from an end state for ai s capabl on the contrari the frontier of artifici intellig have never advanc more rapidli than they are right now as amaz as chatgpt seem to us at the moment it is a mere step stone to what come next what will the next gener of larg languag model llm look like the answer to thi question is alreadi out there under develop at ai startup and research group at thi veri moment more fromforb advisorbest save account of septemb paynecontributorbest interest save account of septemb hortoncontributor thi articl highlight three emerg area that will help defin the next wave of innov in gener ai and llm for those look to remain ahead of the curv in thi on model that can gener their own train data to improv themselv consid how human think and learn we collect knowledg and perspect from extern sourc of by read a book but we also gener novel idea and insight on our own by reflect on a topic or think through a problem in our mind we are abl to deepen our understand of the world via intern reflect and analysi not directli tie to ani new extern input a new avenu of ai research seek to enabl larg languag model to do someth analog effect bootstrap their own intellig as part of their train today s llm ingest much of the world s accumul written inform wikipedia book news articl what if these model onc train could use all the knowledg that they have absorb from these sourc to produc new written then use that content as addit train data in order to improv themselv initi work suggest that thi approach may be power in one recent research effort aptli titl larg languag model can a group of googl research built an llm that can come up with a set of question gener detail answer to those question filter it own answer for the most output and then itself on the curat answer remark thi lead to new perform on variou languag task for instanc the model s perform increas from to on and from to on drop two popular benchmark use to evalu llm perform anoth recent work build on an import llm method call instruct which lie at the core of product like chatgpt wherea chatgpt and other instruct model reli on instruct thi research group built a model that can gener it own natur languag instruct and then itself on those instruct the perform gain are dramat thi method improv the perform of the base model by nearli match the perform of openai s own model in a themat relat work research from googl and carnegi mellon show that if a larg languag model when present with a question first recit to itself what it know about the topic befor respond it provid more accur and sophist respons thi can be loos analog to a human in convers who rather than blurt out the first thing that come to mind on a topic search her memori and reflect on her belief befor share a perspect when peopl first hear about thi line of research a conceptu object often t thi all circular how can a model produc data that the model can then consum to improv itself if the new data came from the model in the first place shouldn t the knowledg or signal that it contain alreadi be incorpor in the model thi object make sens if we conceiv of larg languag model as databas store inform from their train data and reproduc it in differ combin when prompt or even eeri as it may are better off instead conceiv of larg languag model along the line of the human brain no the analog is of cours not perfect we human ingest a tremend amount of data from the world that alter the neural connect in our brain in imponder innumer way through introspect write just a good night s brain can then produc new insight that had not previous been in our mind nor in ani inform sourc out in the world if we intern these new insight they can make us wiser the idea that llm can gener their own train data is particularli import in light of the fact that the world may soon run out of text train data thi is not yet a wide appreci problem but it is one that mani ai research are worri about by one estim the world s total stock of usabl text data is between trillion and trillion token thi includ all the world s book all scientif paper all news articl all of wikipedia all publicli avail code and much of the rest of the internet filter for qualiti webpag blog social media anoth recent estim put the total figur at trillion token deepmind s chinchilla one of today s lead llm wa train on trillion token in other word we may be well within one order of magnitud of exhaust the world s entir suppli of use languag train data if larg languag model are abl to gener their own train data and use it to continu thi could render irrelev the loom data shortag it would repres a leap forward for llm model that can themselv a popular narr these day is that chatgpt and convers llm like it are on the verg of replac googl search as the world s sourc for inform disrupt the tech giant like blockbust or kodak were disrupt befor it thi narr badli oversimplifi thing llm as they exist today will never replac googl search whi not in short becaus today s llm make stuff up as power as they are larg languag model regularli produc inaccur mislead or fals inform and present it confid and convincingli exampl abound of chatgpt s hallucin as these misstat are refer to thi is not to singl out chatgpt everi gener languag model in exist today hallucin in similar way to give a few exampl it recommend book that don t exist it insist that the number is less than it is unsur whether abraham lincoln s assassin wa on the same contin as lincoln at the time of the assassin it provid but incorrect explan of concept like bay theorem most user will not accept a search engin that get basic fact like these wrong some of the time even accuraci will not be good enough for broad market adopt openai ceo sam altman himself acknowledg thi recent caution chatgpt is incred limit but good enough at some thing to creat a mislead impress of great it a mistak to be reli on it for anyth import right it is an open question whether llm hallucin problem can be solv via increment improv to exist architectur or whether a more fundament paradigm shift in ai methodolog will be necessari to give ai common sens and real understand deep learn pioneer yann lecun for one believ the latter lecun s contrarian perspect may prove correct time will tell in the nearer term though a set of promis innov offer to at least mitig llm factual unreli these new method will play an essenti role in prepar llm for widespread deploy two relat capabl lie at the heart of current effort to make languag model more accur the abil for llm to retriev inform from extern sourc and the abil for llm to provid refer and citat for the inform they provid chatgpt is limit to the inform that is alreadi store insid of it captur in it static weight thi is whi it is not abl to discuss event that occur after when the model wa train be abl to pull in inform from extern sourc will empow llm to access the most accur and inform avail even when that inform chang frequent say compani stock price of cours have access to an extern inform sourc doe not by itself guarante that llm will retriev the most accur and relev inform one import way for llm to increas transpar and trust with human user is to includ refer to the sourc s from which they retriev the inform such citat allow human user to audit the inform sourc as need in order to decid for themselv on it reliabl import earli work in thi field includ model like realm from googl and rag from facebook both publish in with the rise of convers llm in recent month research in thi area is now rapidli acceler last year openai publish a version of it gpt model name webgpt that can brows the internet use microsoft bing in order to provid more accur and respons to prompt webgpt navig the internet much like a human doe it can submit search queri to bing follow link scroll up and down on webpag and use function like to find term when the model find relev inform on the internet that it incorpor into it output it provid citat so that the human user can see where the inform came from the result are encourag for the same queri webgpt s respons are prefer to respons written by human subject of the time and are prefer to the respons on reddit of the time deepmind is also pursu research along these line a few month ago deepmind publish a new model name sparrow like chatgpt sparrow is like webgpt it can search the internet for inform and provid citat for it assert sparrow build on import earlier work out of deepmind includ spalm retro and sparrow model in action as shown here sparrow provid quotat and link to support it statement increas their accuraci and deepmind the deepmind research find that sparrow s citat are help and accur of the both that thi research approach is promis and that the problem of llm inaccuraci is far from solv younger startup includ and perplex have also recent launch convers search interfac with the abil to retriev inform from extern sourc and cite refer these product are avail for public use today llm greatest shortcom is their unreli their stubborn tendenc to confid provid inaccur inform languag model promis to reshap everi sector of our economi but they will never reach their full potenti until thi problem is address expect to see plenti of activ and innov in thi area in the month ahead massiv spars expert model today s most promin larg languag model all have effect the same architectur meta ai chief yann lecun said recent in term of underli techniqu chatgpt is not particularli innov it s noth revolutionari although that s the way it s perceiv in the public it s just that you know it s well put togeth it s nice lecun s statement stir up plenti of controversi and twitter debat but the simpl fact is that he is correct as no seriou ai research would disput all of today s languag from openai palm or lamda from googl galactica or opt from meta from from built in the same basic way they are autoregress dens activ model to be sure variat among these model exist their size paramet count the data they are train on the optim algorithm use the batch size the number of hidden layer whether they are instruct and so on these variat can translat to meaning perform differ the core architectur though vari littl yet momentum is build behind an intriguingli differ architectur approach to languag model known as spars expert model while the idea ha been around for decad it ha onli recent reemerg and begun to gain in popular all of the model mention abov are dens thi mean that everi time the model run everi singl one of it paramet is use everi time you submit a prompt to for instanc all billion of the model s paramet are activ in order to produc it respons but what if a model were abl to call upon onli the most relev subset of it paramet in order to respond to a given queri thi is the basic concept behind spars expert model the defin characterist of spars model is that they don t activ all of their paramet for a given input but rather onli those paramet that are help in order to handl the input model sparsiti thu decoupl a model s total paramet count from it comput requir thi lead to spars expert model key advantag they can be both larger and less comput demand than dens model whi are they call spars expert model becaus spars model can be thought of as consist of a collect of that serv as expert on differ topic depend on the prompt present to the model the most relev expert within the model are activ while the other expert remain inact a prompt pose in russian for instanc would onli activ the expert within a model that can understand and respond in russian effici bypass the rest of the model all of today s largest llm are spars if you come across an llm with more than trillion paramet you can safe assum that it is spars thi includ googl s switch transform trillion paramet googl s glam trillion paramet and meta s mixtur of expert model trillion paramet much of the recent progress in ai ha come from train larger and larger model said mikel artetx who led meta s research on spars model befor resign to cofound a stealth llm startup for instanc is more than time larger than but when we doubl the size of a dens model we also make it twice as slow spars model allow us to train larger model without the increas in recent research on spars expert model suggest that thi architectur hold massiv potenti glam a spars expert model develop last year by googl is time larger than requir less energi to train requir half as much comput for infer and outperform on a wide rang of natur languag task similar work on spars model out of meta ha yield similarli promis result as the meta research summar we find that spars model can achiev similar downstream task perform as dens model at a fraction of the comput for model with rel modest comput budget a spars model can perform on par with a dens model that requir almost four time as much there is anoth benefit of spars expert model that is worth mention they are more interpret than dens model abil for a human to understand whi a model took the action that it one of ai s greatest weak today in gener today s neural network are uninterpret black thi can limit their use in the real world particularli in set like healthcar where human review is import spars expert model lend themselv more natur to interpret than convent model becaus a spars model s output is the result of an identifi discret subset of paramet within the the expert that were activ the fact that human can better extract understand explan from spars model about their behavior may prove to be a decis advantag for these model in applic spars expert model are not in widespread use today they are less well understood and more technic complex to build than dens model yet consid their potenti advantag most of all their comput effici don t be surpris to see the spars expert architectur becom more preval in the world of llm go forward in the word of graphcor cto simon knowl if an ai can do mani thing it doesn t need to access all of it knowledg to do one thing it s complet obviou thi is how your brain work and it s also how an ai ought to work i d be surpris if by next year anyon is build dens languag note the author is a partner at radic ventur which is an investor in follow me on twitter rob toewseditori standardsprintreprint permiss'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preProcessor(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42070527",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi='''गोवा की यात्रा बहुत अच्छी रही।\n",
    "समुद्र तट बहुत गर्म थे।\n",
    "मुझे समुद्र तट पर खेलने में बहुत मजा आया।\n",
    "मेरी बेटी बहुत गुस्से में थी।'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09335a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessorHindi(content):\n",
    "    \n",
    "#     #lower case\n",
    "#     content=content.lower()\n",
    "    \n",
    "    #tokenize \n",
    "    content=nltk.word_tokenize(content)\n",
    "    \n",
    "    \n",
    "    #Remove spl chars\n",
    "    y=[]\n",
    "    for i in content:\n",
    "        if i.isalpha():\n",
    "            y.append(i)\n",
    "    \n",
    "    \n",
    "    content=y\n",
    "    y=[]\n",
    "    # remove stopwords : https://www.tutorialspoint.com/Removing-stop-words-with-NLTK-in-Python\n",
    "    for i in content:\n",
    "        if i not in stopwords.words('hindi') or i not in string.punctuation:\n",
    "            y.append(i)\n",
    "    \n",
    "    \n",
    "    #stemming reference:https://www.techtarget.com/searchenterpriseai/definition/stemming#:~:text=Stemming%20is%20the%20process%20of,natural%20language%20processing%20(NLP).\n",
    "    content=y\n",
    "    y=[]\n",
    "    stemmer=PorterStemmer()\n",
    "    for i in content:\n",
    "        y.append(stemmer.stem(i))\n",
    "    return \" \".join(y)\n",
    "    \n",
    "    #lemmatizer\n",
    "    ylem=[]\n",
    "    for i in y:\n",
    "        ylem.append(lemmatizer.lemmatize(i))\n",
    "    return ' '.join(ylem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25467ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import indian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65c92a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in hindi:\n",
    "    if i.isalpha():\n",
    "        y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7809455",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\STUDENT\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\hindi.pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m hindi:\n\u001b[1;32m----> 2\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhindi.pos\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation:\n\u001b[0;32m      3\u001b[0m             y\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(f) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root\u001b[38;5;241m.\u001b[39mjoin(file)\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_path):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\STUDENT\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\hindi.pos'"
     ]
    }
   ],
   "source": [
    "for i in hindi:\n",
    "        if i not in stopwords.words('hindi.pos') or i not in string.punctuation:\n",
    "            y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951e7f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\STUDENT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import indian\n",
    "from nltk.tag import tnt\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download()\n",
    "\n",
    "tagged_set = 'hindi.pos'\n",
    "word_set = indian.sents(tagged_set)\n",
    "count = 0\n",
    "for sen in word_set:\n",
    "    count = count + 1\n",
    "    sen = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in sen]).strip()\n",
    "    print (sen)\n",
    "print (count)\n",
    "\n",
    "train_perc = .9\n",
    "\n",
    "train_rows = int(train_perc*count)\n",
    "test_rows = train_rows + 1\n",
    "\n",
    "print (train_rows, test_rows)\n",
    "\n",
    "data = indian.tagged_sents(tagged_set)\n",
    "train_data = data[:train_rows] \n",
    "test_data = data[test_rows:]\n",
    "\n",
    "\n",
    "pos_tagger = tnt.TnT()\n",
    "pos_tagger.train(train_data)\n",
    "pos_tagger.evaluate(test_data)\n",
    "\n",
    "word_to_be_tagged = u\"३९ गेंदों में दो चौकों और एक छक्के की मदद से ३४ रन बनाने वाले परोरे अंत तक आउट नहीं हुए ।\"\n",
    "\n",
    "tokenized = nltk.word_tokenize(word_to_be_tagged)\n",
    "\n",
    "\n",
    "print(pos_tagger.tag(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7574dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
